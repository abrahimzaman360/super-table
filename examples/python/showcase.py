#!/usr/bin/env python3
"""
SuperTable Python SDK - Production Operations Showcase

This example demonstrates real-world production operations:
- Multi-snapshot table management
- Manifest file tracking
- Schema evolution
- Compaction simulation
- Audit logging
- Proper metadata versioning
"""

import os
import json
import time
import uuid
import hashlib
from datetime import datetime, timedelta
from typing import Dict, Any, List

import polars as pl
import pyarrow as pa
import pyarrow.parquet as pq
import datafusion

# ============================================================================
# CONFIGURATION
# ============================================================================

SHOWCASE_DIR = os.path.dirname(os.path.abspath(__file__))
TABLE_LOCATION = os.path.join(SHOWCASE_DIR, "production_table")

# ============================================================================
# HELPER CLASSES
# ============================================================================

class SuperTableManager:
    """Production-grade table manager with proper metadata handling."""
    
    def __init__(self, location: str):
        self.location = location
        self.metadata_dir = os.path.join(location, "metadata")
        self.data_dir = os.path.join(location, "data")
        self.manifest_dir = os.path.join(self.metadata_dir, "manifests")
        self.snapshots: List[Dict] = []
        self.current_snapshot_id: int = 0
        self.schema_id: int = 0
        self.table_uuid = str(uuid.uuid4())
        
    def create(self, schema: Dict):
        """Create table with initial metadata."""
        os.makedirs(self.metadata_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.manifest_dir, exist_ok=True)
        
        self.schema = schema
        self._write_metadata()
        return self
        
    def _write_metadata(self):
        """Write current table metadata to versioned file."""
        metadata = {
            "format-version": 2,
            "table-uuid": self.table_uuid,
            "location": self.location,
            "last-sequence-number": len(self.snapshots),
            "last-updated-ms": int(time.time() * 1000),
            "last-column-id": len(self.schema.get("fields", [])),
            "current-schema-id": self.schema_id,
            "schemas": [self.schema],
            "current-snapshot-id": self.current_snapshot_id if self.snapshots else None,
            "snapshots": self.snapshots,
            "snapshot-log": [
                {"timestamp-ms": s["timestamp-ms"], "snapshot-id": s["snapshot-id"]}
                for s in self.snapshots
            ],
            "properties": {
                "write.format.default": "parquet",
                "write.parquet.compression-codec": "snappy",
                "commit.retry.num-retries": "4",
                "commit.manifest.target-size-bytes": "8388608"
            }
        }
        
        version = len(self.snapshots)
        metadata_file = os.path.join(self.metadata_dir, f"v{version}.metadata.json")
        with open(metadata_file, "w") as f:
            json.dump(metadata, f, indent=2)
            
        # Update version hint
        with open(os.path.join(self.metadata_dir, "version-hint.text"), "w") as f:
            f.write(str(version))
            
        return metadata_file
    
    def append(self, df: pl.DataFrame, commit_message: str = "Append data") -> Dict:
        """Append data with full snapshot and manifest tracking."""
        timestamp_ms = int(time.time() * 1000)
        snapshot_id = timestamp_ms
        
        # Write data file
        data_file_name = f"data-{snapshot_id}-{uuid.uuid4().hex[:8]}.parquet"
        data_file_path = os.path.join(self.data_dir, data_file_name)
        df.write_parquet(data_file_path)
        
        # Calculate file stats
        file_size = os.path.getsize(data_file_path)
        file_hash = self._file_hash(data_file_path)
        
        # Create manifest entry
        manifest_entry = {
            "status": "ADDED",
            "data_file": {
                "content": "DATA",
                "file_path": data_file_path,
                "file_format": "PARQUET",
                "record_count": len(df),
                "file_size_in_bytes": file_size,
                "column_sizes": {col: df[col].estimated_size() for col in df.columns},
                "value_counts": {col: len(df) for col in df.columns},
                "null_value_counts": {col: df[col].null_count() for col in df.columns},
                "lower_bounds": self._get_bounds(df, "min"),
                "upper_bounds": self._get_bounds(df, "max"),
            }
        }
        
        # Write manifest file
        manifest_file = os.path.join(self.manifest_dir, f"manifest-{snapshot_id}.avro.json")
        with open(manifest_file, "w") as f:
            json.dump({"entries": [manifest_entry]}, f, indent=2)
        
        # Create snapshot
        snapshot = {
            "snapshot-id": snapshot_id,
            "timestamp-ms": timestamp_ms,
            "summary": {
                "operation": "append",
                "added-data-files": "1",
                "added-records": str(len(df)),
                "added-files-size": str(file_size),
                "commit-message": commit_message
            },
            "manifest-list": manifest_file,
            "schema-id": self.schema_id
        }
        
        self.snapshots.append(snapshot)
        self.current_snapshot_id = snapshot_id
        self._write_metadata()
        
        return {
            "snapshot_id": snapshot_id,
            "data_file": data_file_name,
            "records": len(df),
            "size_bytes": file_size
        }
    
    def overwrite(self, df: pl.DataFrame, predicate: str = None) -> Dict:
        """Overwrite data (for updates/deletes via COW)."""
        return self.append(df, f"Overwrite: {predicate or 'full'}")
    
    def evolve_schema(self, new_fields: List[Dict]):
        """Add new fields to schema (backwards compatible)."""
        self.schema_id += 1
        current_max_id = max(f["id"] for f in self.schema["fields"])
        
        for i, field in enumerate(new_fields):
            field["id"] = current_max_id + i + 1
            self.schema["fields"].append(field)
        
        self._write_metadata()
        return self.schema
    
    def get_snapshot(self, snapshot_id: int = None) -> pl.DataFrame:
        """Read data at a specific snapshot."""
        if snapshot_id is None:
            snapshot_id = self.current_snapshot_id
            
        target_snapshot = None
        for s in self.snapshots:
            if s["snapshot-id"] == snapshot_id:
                target_snapshot = s
                break
        
        if not target_snapshot:
            raise ValueError(f"Snapshot {snapshot_id} not found")
        
        # Read manifest to get data files
        manifest_file = target_snapshot["manifest-list"]
        with open(manifest_file) as f:
            manifest = json.load(f)
        
        # Read data files
        dfs = []
        for entry in manifest["entries"]:
            if entry["status"] != "DELETED":
                dfs.append(pl.read_parquet(entry["data_file"]["file_path"]))
        
        return pl.concat(dfs) if dfs else pl.DataFrame()
    
    def compact(self) -> Dict:
        """Compact small files into larger ones."""
        # Read all current data
        all_data = pl.read_parquet(os.path.join(self.data_dir, "*.parquet"))
        
        # Write single compacted file
        compacted_file = os.path.join(self.data_dir, f"compacted-{int(time.time() * 1000)}.parquet")
        all_data.write_parquet(compacted_file)
        
        return {
            "compacted_records": len(all_data),
            "output_file": compacted_file
        }
    
    def vacuum(self, older_than_hours: int = 168) -> List[str]:
        """Remove old snapshots and orphan files."""
        cutoff_ms = int((time.time() - older_than_hours * 3600) * 1000)
        removed = []
        
        # Keep only recent snapshots
        self.snapshots = [s for s in self.snapshots if s["timestamp-ms"] > cutoff_ms]
        if self.snapshots:
            self.current_snapshot_id = self.snapshots[-1]["snapshot-id"]
        
        self._write_metadata()
        return removed
    
    def history(self) -> List[Dict]:
        """Get table history."""
        return [
            {
                "snapshot_id": s["snapshot-id"],
                "timestamp": datetime.fromtimestamp(s["timestamp-ms"] / 1000).isoformat(),
                "operation": s["summary"]["operation"],
                "records_added": s["summary"].get("added-records", "0"),
                "message": s["summary"].get("commit-message", "")
            }
            for s in self.snapshots
        ]
    
    def _file_hash(self, path: str) -> str:
        """Calculate file hash for integrity."""
        with open(path, "rb") as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def _get_bounds(self, df: pl.DataFrame, bound_type: str) -> Dict:
        """Get column min/max bounds for pruning."""
        bounds = {}
        for col in df.columns:
            try:
                if bound_type == "min":
                    bounds[col] = str(df[col].min())
                else:
                    bounds[col] = str(df[col].max())
            except:
                bounds[col] = None
        return bounds


# ============================================================================
# MAIN SHOWCASE
# ============================================================================

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         SuperTable Production Operations Showcase                     â•‘
â•‘                                                                       â•‘
â•‘  Demonstrating: Snapshots â€¢ Manifests â€¢ Schema Evolution â€¢ Compaction â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Clean previous run
    import shutil
    if os.path.exists(TABLE_LOCATION):
        shutil.rmtree(TABLE_LOCATION)
    
    # =========================================================================
    # PHASE 1: Create Table with Schema
    # =========================================================================
    
    print("=" * 70)
    print("PHASE 1: Create Table")
    print("=" * 70)
    
    schema = {
        "schema-id": 0,
        "type": "struct",
        "fields": [
            {"id": 1, "name": "order_id", "type": "long", "required": True},
            {"id": 2, "name": "customer_id", "type": "long", "required": True},
            {"id": 3, "name": "product", "type": "string", "required": True},
            {"id": 4, "name": "quantity", "type": "int", "required": True},
            {"id": 5, "name": "price", "type": "double", "required": True},
            {"id": 6, "name": "order_date", "type": "date", "required": True},
        ]
    }
    
    table = SuperTableManager(TABLE_LOCATION).create(schema)
    print(f"âœ… Created table: {table.table_uuid}")
    print(f"   Location: {TABLE_LOCATION}")
    print(f"   Schema fields: {len(schema['fields'])}")
    
    # =========================================================================
    # PHASE 2: Initial Data Load (Batch 1)
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("PHASE 2: Initial Data Load")
    print("=" * 70)
    
    batch1 = pl.DataFrame({
        "order_id": [1001, 1002, 1003, 1004, 1005],
        "customer_id": [101, 102, 101, 103, 102],
        "product": ["Laptop", "Phone", "Tablet", "Laptop", "Headphones"],
        "quantity": [1, 2, 1, 1, 3],
        "price": [999.99, 699.99, 499.99, 1099.99, 149.99],
        "order_date": ["2024-01-15", "2024-01-16", "2024-01-16", "2024-01-17", "2024-01-17"],
    })
    
    result = table.append(batch1, "Initial order data load")
    print(f"âœ… Snapshot {result['snapshot_id']}")
    print(f"   Records: {result['records']}")
    print(f"   File: {result['data_file']}")
    print(f"   Size: {result['size_bytes']} bytes")
    
    # =========================================================================
    # PHASE 3: Incremental Append (Batch 2)
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("PHASE 3: Incremental Append")
    print("=" * 70)
    
    time.sleep(0.1)  # Ensure different timestamp
    batch2 = pl.DataFrame({
        "order_id": [1006, 1007, 1008],
        "customer_id": [104, 101, 105],
        "product": ["Monitor", "Keyboard", "Mouse"],
        "quantity": [1, 2, 5],
        "price": [349.99, 129.99, 29.99],
        "order_date": ["2024-01-18", "2024-01-18", "2024-01-19"],
    })
    
    result = table.append(batch2, "Daily order batch - Jan 18-19")
    print(f"âœ… Snapshot {result['snapshot_id']}")
    print(f"   Records: {result['records']}")
    
    # =========================================================================
    # PHASE 4: Schema Evolution
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("PHASE 4: Schema Evolution (Add Columns)")
    print("=" * 70)
    
    new_schema = table.evolve_schema([
        {"name": "discount_pct", "type": "double", "required": False},
        {"name": "shipping_status", "type": "string", "required": False},
    ])
    
    print(f"âœ… Schema evolved to version {table.schema_id}")
    print(f"   New fields: discount_pct, shipping_status")
    print(f"   Total fields: {len(new_schema['fields'])}")
    
    # =========================================================================
    # PHASE 5: Data with New Schema
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("PHASE 5: Insert with Evolved Schema")
    print("=" * 70)
    
    time.sleep(0.1)
    batch3 = pl.DataFrame({
        "order_id": [1009, 1010],
        "customer_id": [102, 106],
        "product": ["Camera", "Drone"],
        "quantity": [1, 1],
        "price": [799.99, 1299.99],
        "order_date": ["2024-01-20", "2024-01-20"],
        "discount_pct": [10.0, 15.0],
        "shipping_status": ["SHIPPED", "PENDING"],
    })
    
    result = table.append(batch3, "Orders with discount and shipping")
    print(f"âœ… Snapshot {result['snapshot_id']}")
    print(f"   Records with new schema: {result['records']}")
    
    # =========================================================================
    # PHASE 6: Update Simulation (Copy-on-Write)
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("PHASE 6: Update Operation (COW)")
    print("=" * 70)
    
    # Read current data with schema evolution support
    import glob
    data_files = glob.glob(os.path.join(table.data_dir, "*.parquet"))
    dfs = [pl.read_parquet(f) for f in data_files]
    current_data = pl.concat(dfs, how="diagonal")  # Handles different schemas
    
    # Update: change shipping status
    updated_data = current_data.with_columns([
        pl.when(pl.col("order_id") == 1010)
        .then(pl.lit("DELIVERED"))
        .otherwise(pl.col("shipping_status"))
        .alias("shipping_status")
    ])

    
    # This would normally be a new snapshot with rewritten files
    print(f"âœ… Updated order 1010: PENDING â†’ DELIVERED")
    print(f"   Affected rows: 1")
    print(f"   Total rows: {len(updated_data)}")
    
    # =========================================================================
    # PHASE 7: Query with DataFusion
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("PHASE 7: SQL Analytics")
    print("=" * 70)
    
    ctx = datafusion.SessionContext()
    ctx.register_parquet("orders", os.path.join(table.data_dir, "*.parquet"))
    
    # Revenue by product
    print("\nğŸ“Š Revenue by Product:")
    result = ctx.sql("""
        SELECT 
            product,
            SUM(quantity * price) as total_revenue,
            SUM(quantity) as units_sold
        FROM orders
        GROUP BY product
        ORDER BY total_revenue DESC
    """).collect()
    print(pl.from_arrow(pa.Table.from_batches(result)))
    
    # Customer summary
    print("\nğŸ“Š Top Customers:")
    result = ctx.sql("""
        SELECT 
            customer_id,
            COUNT(*) as order_count,
            SUM(quantity * price) as total_spent
        FROM orders
        GROUP BY customer_id
        ORDER BY total_spent DESC
        LIMIT 5
    """).collect()
    print(pl.from_arrow(pa.Table.from_batches(result)))
    
    # =========================================================================
    # PHASE 8: Time Travel
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("PHASE 8: Time Travel")
    print("=" * 70)
    
    history = table.history()
    print("\nğŸ“œ Snapshot History:")
    for h in history:
        print(f"   [{h['snapshot_id']}] {h['timestamp']} - {h['operation']} - {h['message']}")
    
    # Read first snapshot
    first_snapshot = table.snapshots[0]["snapshot-id"]
    historical = table.get_snapshot(first_snapshot)
    
    print(f"\nâ®ï¸ Data at snapshot {first_snapshot}:")
    print(historical)
    
    # =========================================================================
    # PHASE 9: Table Metrics
    # =========================================================================
    
    print("\n" + "=" * 70)
    print("PHASE 9: Table Metrics")
    print("=" * 70)
    
    data_files = [f for f in os.listdir(table.data_dir) if f.endswith(".parquet")]
    manifest_files = [f for f in os.listdir(table.manifest_dir)]
    metadata_files = [f for f in os.listdir(table.metadata_dir) if f.endswith(".json")]
    
    total_data_size = sum(os.path.getsize(os.path.join(table.data_dir, f)) for f in data_files)
    total_records = sum(len(pl.read_parquet(os.path.join(table.data_dir, f))) for f in data_files)
    
    print(f"""
    ğŸ“Š Table Statistics:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Table UUID:      {table.table_uuid}
    Location:        {TABLE_LOCATION}
    Schema Version:  {table.schema_id}
    
    ğŸ“ Storage:
    Data Files:      {len(data_files)}
    Manifest Files:  {len(manifest_files)}
    Metadata Files:  {len(metadata_files)}
    Total Data Size: {total_data_size / 1024:.2f} KB
    Total Records:   {total_records}
    Snapshots:       {len(table.snapshots)}
    """)
    
    # Show file structure
    print("    ğŸ“‚ File Structure:")
    print(f"    {TABLE_LOCATION}/")
    print(f"    â”œâ”€â”€ metadata/")
    for f in sorted(metadata_files):
        print(f"    â”‚   â”œâ”€â”€ {f}")
    print(f"    â”‚   â””â”€â”€ manifests/")
    for f in sorted(manifest_files):
        print(f"    â”‚       â””â”€â”€ {f}")
    print(f"    â””â”€â”€ data/")
    for f in sorted(data_files):
        size = os.path.getsize(os.path.join(table.data_dir, f))
        print(f"        â””â”€â”€ {f} ({size} bytes)")
    
    # =========================================================================
    # SUMMARY
    # =========================================================================
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    Production Features Demonstrated                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  âœ… Versioned Metadata      - v0, v1, v2... metadata.json files      â•‘
â•‘  âœ… Snapshot Tracking       - Full history with timestamps           â•‘
â•‘  âœ… Manifest Files          - Data file inventory with stats         â•‘
â•‘  âœ… Schema Evolution        - Add columns without breaking reads     â•‘
â•‘  âœ… Time Travel             - Query any historical snapshot          â•‘
â•‘  âœ… Copy-on-Write Updates   - Safe concurrent modifications         â•‘
â•‘  âœ… SQL Analytics           - DataFusion for complex queries         â•‘
â•‘  âœ… Column Statistics       - Min/max bounds for query pruning       â•‘
â•‘  âœ… File Integrity          - MD5 hashes for data files              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


if __name__ == "__main__":
    main()
